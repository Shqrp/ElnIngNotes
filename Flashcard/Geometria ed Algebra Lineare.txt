#separator:tab
#html:true
#guid column:1
#deck column:2
#tags column:5
ge}*b3W6=W	Geometria ed Algebra Lineare	Cos'è un campo vettoriale?	Una struttura \((\mathbb{K},+,\cdot)\) si dice <b>campo</b>&nbsp;se le operazioni di somma e prodotto sono binarie interne e soddisfano:<br>1. <b>Proprietà della somma</b>: associativa, commutativa, esistenza dell'elemento neutro \(0\) e dell'inverso additivo.<br>2. <b>Proprietà del prodotto</b>: associativa, esistenza dell'elemento neutro \(1\) e dell'inverso moltiplicativo.<br>3. <b>Proprietà distributiva</b>: del prodotto rispetto alla somma.<br>Esempi: \(\mathbb{R}, \mathbb{Q}, \mathbb{C}\) o campi finiti come \(\mathbb{F}_2\).	Spazi
Oz42Wo)-Fb	Geometria ed Algebra Lineare	Cos'è uno spazio vettoriale?	Uno <b>spazio vettoriale</b> su un campo \(\mathbb{K}\) è un insieme \(V\) di vettori in cui sono definite due operazioni:<br>1. <b>Somma</b> (binaria interna): rende \(V\) un gruppo commutativo (associativa, commutativa, elemento neutro \(\mathcal{O}\), inverso).<br>2. <b>Prodotto per scalare</b> (binaria esterna \(\mathbb{K} \times V \rightarrow V\)): soddisfa la proprietà distributiva (scalare su somma vettori e vettore su somma scalari), associativa mista e l'esistenza dell'elemento neutro \(1\).	Spazi
H7TxNN)M@!	Geometria ed Algebra Lineare	Descrivi le tre operazioni basilari tra vettori.	1. <b>Somma</b>: operazione binaria interna che restituisce un vettore con componenti sommate membro a membro: \(\binom{x_1}{...} + \binom{y_1}{...} = \binom{x_1+y_1}{...}\).<br>2. <b>Prodotto per scalare</b>: operazione binaria esterna che moltiplica ogni componente del vettore per lo scalare dato: \(k \cdot \binom{a}{b} = \binom{ka}{kb}\).<br>3. <b>Prodotto scalare standard</b>: operazione binaria esterna (solo in \(\mathbb{R}\)) che restituisce la somma dei prodotti delle componenti. Se il risultato è nullo, i vettori sono perpendicolari.	Spazi
CxZe%|@{@3	Geometria ed Algebra Lineare	Enuncia e dimostra la condizione di dipendenza lineare di vettori.	<b>Enunciato</b>: \(v_1, ..., v_n\) sono linearmente dipendenti se e solo se uno dei vettori è combinazione lineare degli altri.<br><b>Dimostrazione</b>:<br>(\(\Rightarrow\)) Se dipendenti, esiste \(\sum c_i v_i = 0\) con almeno un \(c_i \neq 0\) (supponiamo \(c_1\)). Isolando \(v_1\) si ha: \(v_1 = -(c_1^{-1}c_2)v_2 - ... - (c_1^{-1}c_n)v_n\), quindi \(v_1 \in \mathcal{L}(v_2, ..., v_n)\).<br>(\(\Leftarrow\)) Se \(v_1 = d_2 v_2 + ... + d_n v_n\), portando \(v_1\) a destra si ottiene \(d_2 v_2 + ... + d_n v_n - v_1 = \mathcal{O}\). Poiché il coefficiente di \(v_1\) è \(-1 \neq 0\), questa è una combinazione lineare non banale che dà zero, quindi sono dipendenti.	Spazi::Basi
uPY({!!=r&	Geometria ed Algebra Lineare	Cos'è una base?	Una <b>base finita</b> di uno spazio vettoriale \(V\) finitamente generato è un insieme ordinato di vettori \(B=\{v_1, ..., v_n\}\) tale che:<br>1. È un generatore di \(V\), ossia \(V=\mathcal{L}(v_1, ..., v_n)\).<br>2. I vettori sono linearmente indipendenti.<br>Ciò implica che ogni vettore \(v \in V\) si scrive in modo <b>unico</b> come combinazione lineare degli elementi della base (le coordinate sono uniche).	Spazi::Basi
nzCQp9_$v^	Geometria ed Algebra Lineare	Enuncia e dimostra il lemma di scarto.	<b>Enunciato</b>: Se \(v_1, ..., v_n\) sono linearmente dipendenti, allora \(\mathcal{L}(v_1, ..., v_n) = \mathcal{L}(v_1, ..., v_{n-1})\) (supponendo, senza perdita di generalità, che \(v_n\) sia dipendente).<br><b>Dimostrazione</b>: Poiché dipendenti, \(v_n\) è combinazione lineare degli altri: \(v_n = \sum_{i=1}^{n-1} c_i v_i\).<br>1. L'inclusione \(\mathcal{L}(v_1...v_{n-1}) \subseteq \mathcal{L}(v_1...v_n)\) è ovvia (basta prendere coefficiente 0 per \(v_n\)).<br>2. Per l'inclusione \(\mathcal{L}(v_1...v_n) \subseteq \mathcal{L}(v_1...v_{n-1})\): ogni vettore \(w = \sum d_i v_i\) generato dall'insieme completo si può riscrivere sostituendo \(v_n\) con la sua espressione in funzione di \(v_1...v_{n-1}\), rimanendo quindi nello span dell'insieme ridotto.	Spazi::Basi
b`)%tf;?t+	Geometria ed Algebra Lineare	Enuncia e dimostra il lemma di aggiunta.	<b>Enunciato</b>: Siano \(v_1, ..., v_l\) linearmente indipendenti. Se \(v_{l+1} \notin \mathcal{L}(v_1, ..., v_l)\), allora \(v_1, ..., v_{l+1}\) sono linearmente indipendenti.<br><b>Dimostrazione</b>: Consideriamo la combinazione lineare \(\sum_{i=1}^{l+1} c_i v_i = \mathcal{O}\). Se per assurdo fosse \(c_{l+1} \neq 0\), potremmo scrivere \(v_{l+1} = \sum_{i=1}^l (-c_i/c_{l+1}) v_i\), il che implicherebbe \(v_{l+1} \in \mathcal{L}(v_1, ..., v_l)\), contraddicendo l'ipotesi. Dunque deve essere \(c_{l+1}=0\). L'equazione si riduce a \(\sum_{i=1}^l c_i v_i = \mathcal{O}\) e, poiché i primi \(l\) vettori sono linearmente indipendenti per ipotesi, tutti i coefficienti \(c_1, ..., c_l\) devono essere nulli.	Spazi::Basi
zHApxS$L~=	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema di esistenza di una base.	<b>Enunciato</b>: Se \(V\) è finitamente generato, allora esiste una base finita.<br><b>Dimostrazione</b>: Sia \(V=\mathcal{L}(v_1, ..., v_n)\).<br>1. Se \(v_1, ..., v_n\) sono linearmente indipendenti, allora formano una base.<br>2. Se sono linearmente dipendenti, per il <b>Lemma di scarto</b> posso eliminare un vettore che è combinazione lineare degli altri (ad esempio \(v_n\)) mantenendo lo stesso span: \(\mathcal{L}(v_1, ..., v_n) = \mathcal{L}(v_1, ..., v_{n-1})\).<br>3. Se i vettori rimasti sono ancora dipendenti, ripeto lo scarto finché non ottengo un insieme di vettori linearmente indipendenti che generano \(V\). Tale insieme sarà la base cercata.	Spazi::Basi
ueE{uk>kqs	Geometria ed Algebra Lineare	Enuncia e dimostra il legame intercorre tra una combinazione lineare di vettori e le sue coordinate?	<b>Enunciato</b>: Le coordinate di una combinazione lineare di vettori corrispondono alla combinazione lineare delle coordinate: \([a u_1 + b u_2]_B = a [u_1]_B + b [u_2]_B\).<br><b>Dimostrazione</b>: Siano \(u_1 = \sum c_i v_i\) e \(u_2 = \sum d_i v_i\) vettori espressi rispetto alla base \(B=\{v_i\}\). Consideriamo la combinazione lineare \(a u_1 + b u_2\). Sostituendo le espressioni dei vettori, otteniamo \(a(\sum c_i v_i) + b(\sum d_i v_i) = \sum (a c_i + b d_i) v_i\). Le coordinate del vettore risultante sono proprio i coefficienti \((a c_i + b d_i)\), che vettorialmente corrispondono a \(a[u_1]_B + b[u_2]_B\).	Spazi::Basi
b/NHb>++U^	Geometria ed Algebra Lineare	Cos'è un sistema di riferimento?	È un metodo per individuare punti nello spazio affine. Fissato un punto di origine \(O\), ogni punto \(P\) è individuato univocamente dal vettore \(OP\).<br>- In \(\mathbb{A}_1\) (retta): serve un vettore direttore \(v\). \(OP \simeq (coordinate)\).<br>- In \(\mathbb{A}_2\) (piano): servono due vettori \(v, u\) non paralleli. \(OP = t v + s u\).<br>- In \(\mathbb{A}_3\) (spazio): servono tre vettori \(v, u, w\) non complanari.<br>Spesso si usano sistemi di riferimento ortonormali (vettori di norma 1 e perpendicolari tra loro) per semplificare i calcoli (es. teorema di Pitagora, coseno dell'angolo).	GeometriaAnalitica
KOVb52x^9T	Geometria ed Algebra Lineare	Che forme può assumere l'equazione di una retta?	1. <b>Parametrica</b>: \(P = P_0 + t v\), dove \(P_0\) è un punto noto, \(v\) il vettore direttore e \(t \in \mathbb{R}\).<br>2. <b>Cartesiana</b>: In \(\mathbb{A}_3\), la retta è definita come intersezione di due piani non paralleli, espressa dal sistema \(\{ax+by+cz+d=0, \ ex+fy+gz+h=0\}\).	GeometriaAnalitica
kVHFslB8)&	Geometria ed Algebra Lineare	Che forme può assumere l'equazione di un piano?	1. <b>Parametrica</b>: \(P = P_0 + t v + s u\), con \(v, u\) vettori linearmente indipendenti (base della giacitura) e \(t, s \in \mathbb{R}\).<br>2. <b>Cartesiana</b>: \(ax+by+cz+d=0\), dove \((a,b,c)\) rappresenta un vettore direttore perpendicolare (normale) a ogni vettore appartenente al piano.	GeometriaAnalitica
K:+T]kiFnz	Geometria ed Algebra Lineare	Descrivi le posizioni reciproche nelle quali si possono trovare due rette, due piani e una retta e un piano.	<b>Due Rette</b>: parallele (vettori direttori paralleli), incidenti (intersezione non vuota, un punto), complanari (contenute in un piano: parallele o incidenti), perpendicolari (vettori direttori ortogonali), sghembe (non complanari).<br><b>Due Piani</b>: paralleli (vettori normali paralleli), incidenti (intersezione non vuota, una retta).<br><b>Retta e Piano</b>: paralleli (vettore retta perpendicolare al vettore normale del piano), incidenti (intersezione non vuota).	GeometriaAnalitica
HEmmRVeeK	Geometria ed Algebra Lineare	Cos'è una matrice?	Una <b>matrice</b> \(A \in \mathbb{M}_{m,n}(\mathbb{K})\) con \(m\) righe ed \(n\) colonne è definita formalmente come una funzione \(A: \{1...m\} \times \{1...n\} \rightarrow \mathbb{K}\). È comunemente rappresentata come una tabella di elementi \(a_{ij}\) appartenenti al campo \(\mathbb{K}\). Può essere vista come un insieme ordinato di vettori riga o vettori colonna.	Matrici
F1a;!Ibhv2	Geometria ed Algebra Lineare	Descrivi la somma fra matrici.	Date due matrici \(A, B \in \mathbb{M}_{m,n}(\mathbb{K})\), la <b>somma</b> \(C = A+B\) è definita elemento per elemento: \(c_{ij} = a_{ij} + b_{ij}\).<br>Questa operazione rende l'insieme delle matrici un <b>gruppo commutativo</b>, per cui valgono le proprietà: associativa, commutativa, esistenza dell'elemento neutro \(O_{m,n}\) (matrice nulla) e esistenza dell'inverso \(-A\) (matrice con elementi di segno opposto).	Matrici
"HL<#bPE(jU"	Geometria ed Algebra Lineare	Descrivi il prodotto per uno scalare di una matrice e le sue proprietà.	Dato uno scalare \(b \in \mathbb{K}\) e una matrice \(A \in \mathbb{M}_{m,n}\), il prodotto \(b \cdot A\) restituisce una matrice con elementi \(c_{ij} = b \cdot a_{ij}\).<br><b>Proprietà</b>:<br>1. Distributiva dello scalare sulla somma di matrici.<br>2. Distributiva della matrice sulla somma di scalari.<br>3. Associativa mista \((cd)A = c(dA)\).<br>4. Esistenza dell'elemento neutro \(1 \cdot A = A\) e dell'opposto \(-1 \cdot A = -A\).<br>Queste proprietà rendono \((\mathbb{M}_{m,n}(\mathbb{K}),+,\cdot,\mathbb{K})\) uno spazio vettoriale.	Matrici
F3oIa*s:%:	Geometria ed Algebra Lineare	Descrivi il prodotto fra matrici e le sue proprietà.	Date \(A \in \mathbb{M}_{m,n}\) e \(B \in \mathbb{M}_{n,k}\), il prodotto \(C = A \cdot B \in \mathbb{M}_{m,k}\) ha elementi \(c_{ij}\) calcolati come il prodotto scalare (riga per colonna) tra la riga \(i\)-esima di \(A\) e la colonna \(j\)-esima di \(B\): \(c_{ij} = \sum_{l=1}^n a_{il} b_{lj}\).<br><b>Proprietà</b>:<br>1. \(A \cdot e_k = C_k\) (la k-esima colonna).<br>2. \(A \cdot I = A\) (elemento neutro).<br>3. Distributiva (destra e sinistra) rispetto alla somma.<br>4. Associativa.<br>5. <b>Non</b> vale la proprietà commutativa.<br>6. Trasposta del prodotto: \((AB)^t = B^t A^t\) (inversione dell'ordine).	Matrici
tE-mjkY_aj	Geometria ed Algebra Lineare	Cos'è il nucleo di una matrice?	Il <b>nucleo</b> (o kernel) di una matrice \(A \in \mathbb{M}_{m,n}(\mathbb{K})\) è l'insieme delle soluzioni del sistema lineare omogeneo associato: \(ker(A) = Sol(A, \underline{0}_m) = \{x \in \mathbb{K}^n : A \cdot x = \underline{0}_m\}\).<br>Il nucleo contiene sempre il vettore nullo \(\underline{0}_n\) e, se \(n &gt; m\), contiene sicuramente vettori non nulli (dipende da almeno un parametro).	SistemiLineari
p)-WlO2Z!1	Geometria ed Algebra Lineare	Cosa sono le mosse di Gauss? Dimostra che preservano l'insieme delle soluzioni.	Le <b>mosse di Gauss</b> sono operazioni sulle righe di una matrice: 1. Scambio di due righe. 2. Sostituzione di una riga con una combinazione lineare: \(R_i \rightarrow a R_i + c R_k\) con \(a \neq 0\).<br><b>Dimostrazione conservazione soluzioni</b>: La mossa 1 è ovvia. Per la mossa 2: se un vettore \(\alpha\) è soluzione del sistema originale, soddisfa \(R_i \cdot \alpha = b_i\) e \(R_k \cdot \alpha = b_k\), quindi soddisfa anche la loro combinazione lineare (la nuova riga). Viceversa, poiché \(a \neq 0\), la trasformazione è invertibile (posso tornare alla riga originale sottraendo), quindi se \(\alpha\) risolve il nuovo sistema, risolve anche quello di partenza.	SistemiLineari
n{uWLh/pw8	Geometria ed Algebra Lineare	Descrivi gli algoritmi di Gauss e di Gauss-Jordan.	<b>Algoritmo di Gauss</b> (per ottenere matrice a scala):<br>1. Si individua la prima colonna non nulla e si scambia una riga affinché il primo pivot sia non nullo (\(a_{i_1 j_1} \neq 0\)).<br>2. Si rendono 0 tutti i coefficienti sotto il pivot tramite la mossa di Gauss.<br>3. Si ripete il procedimento considerando la sottomatrice che esclude la prima riga e le colonne fino a quella del pivot.<br><b>Algoritmo di Gauss-Jordan</b> (per scala ridotta):<br>1. Si applica Gauss per ottenere una matrice a scala.<br>2. Partendo dall'ultimo pivot, si annullano tutti i valori <i>sopra</i> di esso con le mosse di Gauss.<br>3. Se ci sono altri pivot sopra non sistemati, si ripete.<br>4. Si divide ogni riga per il valore del proprio pivot affinché tutti i pivot valgano 1. L'algoritmo produce un'unica matrice a scala ridotta.	SistemiLineari
j_hg%to,mz	Geometria ed Algebra Lineare	Com'è strutturato un sistema lineare?	Dato il sistema \(A \cdot \alpha = b\), l'insieme delle soluzioni è strutturato come \(Sol(A,b) = \alpha + ker(A)\).<br>Ossia, le soluzioni sono date dalla somma di una <b>soluzione particolare</b> \(\alpha\) e del <b>nucleo</b> della matrice (soluzione del sistema omogeneo associato). Se il sistema è omogeneo, il nucleo è descritto come span di \(n - rk(A)\) vettori linearmente indipendenti.	SistemiLineari
i06!QrKzHR	Geometria ed Algebra Lineare	Enuncia e dimostra le proprietà di un sistema lineare.	1. <b>Linearità</b>: Se \(A v_1 = b_1\) e \(A v_2 = b_2\), allora \(A(c v_1 + d v_2) = c b_1 + d b_2\). Dimostrazione diretta per proprietà distributiva del prodotto matrice-vettore.<br>2. <b>Struttura affine</b>: Se \(\alpha \in Sol(A,b)\), allora \(Sol(A,b) = \alpha + ker(A)\).<br><b>Dimostrazione</b>: Se \(\beta\) è un'altra soluzione, \(A\alpha=b\) e \(A\beta=b\), sottraendo si ha \(A(\alpha-\beta)=\underline{0}\), quindi \(\alpha-\beta \in ker(A)\), da cui \(\alpha = \beta + v\) con \(v \in ker(A)\). Viceversa, se \(\beta = \alpha + v\) con \(v \in ker(A)\), allora \(A(\alpha+v)=A\alpha+Av=b+\underline{0}=b\).	SistemiLineari
ezO.tcKAS!	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema di Rouché-Capelli e il suo corollario.	<b>Teorema</b>: Il sistema \(A \cdot x = b\) ha soluzione se e solo se \(rk(A) = rk(A|b)\). Se esiste soluzione, lo spazio delle soluzioni è \(\alpha + ker(A)\) con dimensione \(n-rk(A)\) (numero di parametri).<br><b>Dimostrazione</b>: Le mosse di Gauss non cambiano il rango né le soluzioni. Riducendo a scala ridotta con Gauss-Jordan, il sistema ha soluzione se e solo se non compare un pivot nella colonna dei termini noti, ossia&nbsp;\(rk(A) = rk(A') = rk (A'|b') = rk(A|b)\)&nbsp;(altrimenti avremmo un'equazione impossibile \(0 = c \neq 0\)).<br><b>Corollario</b>: Se \(m \ge n\), esiste un'unica soluzione \(\Leftrightarrow rk(A)=n\). Se \(m &lt; n\), ci sono infinite soluzioni dipendenti da parametri (se il campo è infinito).	SistemiLineari
e3VcE+I%f<	Geometria ed Algebra Lineare	Cos'è una matrice invertibile? Enuncia e dimostra le sue caratteristiche.	Una matrice quadrata \(A \in \mathbb{M}_{n,n}\) si dice <b>invertibile</b> se esistono un'inversa destra e una sinistra che coincidono.<br><b>Caratteristiche equivalenti</b>:<br>1. \(A\) è invertibile, ossia esistono l'inversa destra&nbsp;\(D\)&nbsp;e sinistra&nbsp;\(S\)<br>2. Il sistema \(A \cdot x = b\) ha un'unica soluzione.<br>3. \(rk(A)=n\).<br><b>Dimostrazione</b>:<br>(\(1 \Rightarrow 2\)) Se esiste \(A^{-1}\), allora \(x=A^{-1} \cdot b\) è l'unica soluzione.<br>(\(2 \Rightarrow 3\)) Per Rouché-Capelli/Cramer, se la soluzione è unica allora \(rk(A)=n\).<br>(\(3 \Rightarrow 1\)) Se \(rk(A)=n\), risolvendo gli \(n\) sistemi lineari \(A \cdot d_i = e_i\) (dove \(e_i\) sono i vettori canonici), si trovano le colonne uniche che compongono la matrice inversa \(D\). Per il teorema di Cramer,&nbsp;\(d_i\)&nbsp;esiste ed è unica.	SistemiLineari
E6i-,2$`X-	Geometria ed Algebra Lineare	Cos'è un sottospazio vettoriale? Quando lo è?	Dato uno spazio vettoriale \(V\), un sottoinsieme \(U \subseteq V\) è un <b>sottospazio vettoriale</b> se valgono in \(U\) le stesse operazioni di \(V\) ristrette a \(U\).<br><b>Criterio</b>: \(U\) è un sottospazio se è chiuso rispetto alla combinazione lineare, ovvero se \(\forall c_1, c_2 \in \mathbb{K}\) e \(\forall u_1, u_2 \in U\), allora \(c_1 u_1 + c_2 u_2 \in U\). (Nota: contiene sempre il vettore nullo).	Sottospazi
"n>3f#t>_t,"	Geometria ed Algebra Lineare	Dimostra che uno span lineare è un sottospazio vettoriale.	Sia \(U = \mathcal{L}(u_1, ..., u_l)\). Consideriamo due vettori \(v_1, v_2 \in U\). Essi sono combinazioni lineari dei generatori: \(v_1 = \sum a_i u_i\) e \(v_2 = \sum b_i u_i\).<br>Consideriamo una loro combinazione lineare \(c v_1 + d v_2\). Sostituendo le espressioni precedenti, otteniamo \(\sum (c a_i + d b_i) u_i\), che è ancora una somma pesata dei generatori \(u_i\). Di conseguenza, il vettore risultante appartiene a \(U\), soddisfacendo il criterio dei sottospazi.	Sottospazi
r$Tw2z^d,4	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema del completamento di una base.	<b>Enunciato</b>: Sia \(U\) un sottospazio e \(u_1, ..., u_l \in U\) vettori linearmente indipendenti. Allora è possibile estendere questo insieme fino a ottenere una base di \(U\).<br><b>Dimostrazione</b>: Sia \(B_l = \{u_1...u_l\}\).<br>1. Se \(\mathcal{L}(B_l)=U\), allora \(B_l\) è già una base.<br>2. Se no, esiste un vettore \(u_{l+1} \in U \setminus \mathcal{L}(B_l)\). Per il <b>Lemma di aggiunta</b>, l'insieme \(B_l \cup \{u_{l+1}\}\) è ancora linearmente indipendente.<br>3. Si itera l'aggiunta finché lo span non coincide con \(U\) (processo finito poiché la dimensione è finita).	Sottospazi
l;vpW289g+	Geometria ed Algebra Lineare	Quando un sottospazio e uno spazio vettoriale coincidono? Enuncia e dimostra la proposizione.	<b>Proposizione</b>: Sia \(U \subseteq V\) un sottospazio. Se \(dim(U)=dim(V)\), allora \(U=V\).<br><b>Dimostrazione</b>: Sia \(n=dim(V)\) e \(u_1, ..., u_n\) una base di \(U\). Questi sono \(n\) vettori linearmente indipendenti in \(V\). Se per assurdo \(U \neq V\), esisterebbe un vettore \(u_{n+1} \in V \setminus U\). Per il Lemma di aggiunta, l'insieme \(\{u_1, ..., u_n, u_{n+1}\}\) sarebbe linearmente indipendente, il che è impossibile perché supererebbe la dimensione massima \(n\) dello spazio \(V\).	Sottospazi
HBs1C/+%JG	Geometria ed Algebra Lineare	Dimostra che le mosse di Gauss non cambiano lo span delle righe.	Sia \(A\) la matrice originale e \(A'\) la matrice ottenuta con la mossa \(R_i \rightarrow a R_i + b R_k\) (con \(a \neq 0\)).<br>Per il Lemma di scarto/combinazione, il nuovo vettore riga è combinazione lineare delle righe di \(A\), quindi lo span delle righe di \(A'\) è contenuto nello span di \(A\).<br>Poiché la mossa è invertibile, vale anche il viceversa: le righe di \(A\) possono essere espresse come combinazione di quelle di \(A'\). Dunque, i due sottospazi generati (span delle righe) coincidono.	Sottospazi
Bv/lHCi__u	Geometria ed Algebra Lineare	Come posso estrarre una base da una matrice del tipo&nbsp;\(B = (v_1 | ... | v_n)\)?	Per estrarre una base dallo span dei vettori colonna di \(B\):<br>1. Si riduce la matrice \(B\) a scala.<br>2. Si identificano gli indici di colonna \(I = \{j_1, ..., j_k\}\) in cui cadono i pivot.<br>3. La base cercata è costituita dai vettori della matrice <b>originale</b> \(B\) corrispondenti a quegli indici: \(\{v_{j_1}, ..., v_{j_k}\}\).	Sottospazi
yY}mJ<^`U%	Geometria ed Algebra Lineare	Dimostra che l'intersezione di sottospazi vettoriali è anch'essa un sottospazio vettoriale.	Siano \(U_1\) e \(U_2\) due sottospazi di \(V\).<br>Prendiamo due vettori \(u_1, u_2\) che appartengono all'intersezione \(U_1 \cap U_2\). Questo significa che \(u_1, u_2 \in U_1\) e \(u_1, u_2 \in U_2\).<br>Poiché \(U_1\) e \(U_2\) sono sottospazi, sono chiusi rispetto alla combinazione lineare. Quindi \(c_1 u_1 + c_2 u_2\) appartiene a \(U_1\) e appartiene a \(U_2\).<br>Di conseguenza, la combinazione lineare appartiene a \(U_1 \cap U_2\), dimostrando che l'intersezione è un sottospazio.<br><br>(In genere, l'unione di due sottospazi non è un sottospazio)&nbsp;	Sottospazi
bj@G)FliJ(	Geometria ed Algebra Lineare	Come posso scrivere lo span della somma di due sottospazi vettoriali?	Se \(H = \mathcal{L}(h_1, ..., h_k)\) e \(W = \mathcal{L}(w_1, ..., w_m)\), allora la somma \(H+W\) è lo span dell'unione dei generatori:<br>\(H+W = \mathcal{L}(h_1, ..., h_k, w_1, ..., w_m)\).<br><b>Dimostrazione</b>: Ogni vettore \(v \in H+W\) si scrive come \(v = h + w\). Sostituendo \(h\) e \(w\) con le loro espressioni come combinazione lineare dei generatori, \(v\) risulta essere una combinazione lineare dell'insieme unito di tutti i generatori.	Sottospazi
Gj9o;6.w|Z	Geometria ed Algebra Lineare	Qual è una base della somma di due sottospazi vettoriali?	Secondo il <b>Lemma dell'unione di basi estese</b>, la base è data dall'unione delle basi di \(H\) e \(W\) costruite estendendo una base dell'intersezione.<br><b>Costruzione</b>: Sia \(B_{H \cap W} = \{u_1, ..., u_l\}\). Si estende a base di \(H\) (\(B_H=\{u_i, h_j\}\)) e a base di \(W\) (\(B_W=\{u_i, w_p\}\)). La base di \(H+W\) è \(B_H \cup B_W\).<br><b>Dimostrazione indipendenza</b>: Sia&nbsp;\(U \subseteq H\)&nbsp;un sottospazio.&nbsp;Consideriamo una combinazione lineare nulla \(u + h + w = \sum a_i u_i + \sum b_j h_j + \sum c_p w_p = \mathcal{O}\). Portando i termini \(w\) a destra, otteniamo un vettore che sta in \(W\) (combinazione di \(w_p\)) ma che è uguale a una somma in \(H\) (combinazione di \(u_i, h_j\)). Dunque questo vettore sta nell'intersezione \(H \cap W\) e si può scrivere come combinazione dei soli \(u_i\). Uguagliando le espressioni, si ottiene una combinazione lineare di soli elementi della base \(B_H\) (indipendenti), il che forza i coefficienti \(b_j\) a essere 0. Ripetendo per \(B_W\), anche \(c_p\) sono 0. Infine, anche gli \(a_i\) devono essere 0.	Sottospazi
sVpJJae_IN	Geometria ed Algebra Lineare	Cos'è la somma diretta di due sottospazi?	La somma di due sottospazi \(H\) e \(W\) si dice <b>somma diretta</b> (indicata con \(H \oplus W\)) se la loro intersezione contiene solamente il vettore nullo: \(H \cap W = \{\mathcal{O}\}\).<br>Questo implica che ogni vettore della somma si può scrivere in modo <i>unico</i> come somma di un elemento di \(H\) e uno di \(W\).	Sottospazi
sbVR0Z%Tm%	Geometria ed Algebra Lineare	Enuncia e dimostra la formula di Grassmann.	<b>Formula</b>: \(dim(H+W) = dim(H) + dim(W) - dim(H \cap W)\).<br><b>Dimostrazione</b>: Sia \(B_{H \cap W} = \{u_1, ..., u_l\}\) una base dell'intersezione. Estendiamola a una base di \(H\) aggiungendo \(\{h_{l+1}, ..., h_m\}\) e a una base di \(W\) aggiungendo \(\{w_{l+1}, ..., w_k\}\).<br>Si dimostra che l'unione di tutti questi vettori \(\{u_i, h_j, w_p\}\) è una base di \(H+W\).<br>Contando gli elementi: \(dim(H+W) = l + (m-l) + (k-l) = m + k - l\), che corrisponde esattamente alla somma delle dimensioni meno la dimensione dell'intersezione.	Sottospazi
GPWf.;Z,sc	Geometria ed Algebra Lineare	Cos'è un'applicazione o funzione lineare? Cosa si intende per endomorfismo e isomorfismo?	Una funzione \(T: V \rightarrow W\) tra due spazi vettoriali è <b>lineare</b> se soddisfa due proprietà per ogni \(u, v \in V\) e \(c \in \mathbb{K}\):<br>1. Additività: \(T(v+u) = T(v) + T(u)\).<br>2. Omogeneità: \(T(c \cdot v) = c \cdot T(v)\).<br><b>Endomorfismo</b>: se dominio e codominio coincidono (\(V=W\)).<br><b>Isomorfismo</b>: se la funzione lineare è biunivoca.	ApplicazioniLineari
"QLz1p#On4@"	Geometria ed Algebra Lineare	Descrivi le proprietà di una funzione lineare.	1. \(T(\mathcal{O}_V) = \mathcal{O}_W\) (manda il nullo nel nullo).<br>2. \(T(-v) = -T(v)\).<br>3. Conserva le combinazioni lineari: \(T(\sum c_i v_i) = \sum c_i T(v_i)\).<br>4. La composizione di funzioni lineari è lineare.<br>5. L'immagine di un sottospazio è un sottospazio di \(W\).<br>6. La controimmagine di un sottospazio è un sottospazio di \(V\).	ApplicazioniLineari
kL(Z1IEj?J	Geometria ed Algebra Lineare	Quando posso esprimere una funzione lineare come span lineare?	Se \(U = \mathcal{L}(u_{1},...,u_{k})\) allora \(T(U) = \mathcal{L}(T(u_{1}), ..., T(u_{k}))\).<br><br><b>Dimostrazione:</b><br>Sia \(u \in U\). Dunque \(u = \sum_{i=1}^{k}c_{i}u_{i}\).<br>Per la linearità di T:<br>\(\Rightarrow T(u) = T\left(\sum_{i=1}^{k}c_{i}u_{i}\right) = \sum_{i=1}^{k}c_{i}T(u_{i}) \in \mathcal{L}(T(u_{1}), ..., T(u_{k}))\).<br><br>Viceversa, sia \(w \in \mathcal{L}(T(u_{1}), ..., T(u_{k}))\).<br>Dunque \(w = \sum_{i=1}^{k}d_{i}T(u_{i})\).<br>\(\Rightarrow w = T\left(\sum_{i=1}^{k}d_{i}u_{i}\right)\). Posto \(u' = \sum_{i=1}^{k}d_{i}u_{i} \in U\), si ha \(w = T(u') \in T(U)\).	ApplicazioniLineari
"f#oF,,fb0;"	Geometria ed Algebra Lineare	La controimmagine di un vettore è un sottospazio vettoriale?	In generale <b>no</b>, a meno che il vettore non sia il vettore nullo \(\mathcal{O}_W\).<br>La controimmagine \(T^{-1}(w)\) è un <b>sottospazio affine</b> della forma \(\alpha + ker(T)\), dove \(\alpha\) è una soluzione particolare tale che \(T(\alpha)=w\). Se \(w \neq \mathcal{O}\), l'insieme non contiene lo zero e non è chiuso rispetto alla somma (la differenza di due controimmagini finisce nel nucleo).	ApplicazioniLineari
c`+6pDKKtz	Geometria ed Algebra Lineare	Cos'è il nucleo di una funzione lineare? Enuncia e dimostra le sue proprietà.	Il <b>nucleo</b> \(ker(T)\) è l'insieme dei vettori di \(V\) che vengono mappati nel vettore nullo di \(W\): \(ker(T) = \{v \in V : T(v) = \mathcal{O}_W\}\).<br><b>Proprietà</b>:<br>1. È un sottospazio vettoriale.<br>2. \(ker(T) = \{\mathcal{O}_V\} \Leftrightarrow T\) è iniettiva.<br>3. (Teorema della fibra) Se per ogni&nbsp;\(w \in W\)&nbsp;\(T^{-1} (w) \neq \emptyset\), allora&nbsp;\(T^{-1} (w) = \alpha + ker(T)\)&nbsp;con&nbsp;\(\alpha \in T^{-1} (w)\)<br>4. Se&nbsp;\(H = \mathcal{L}(h_1, ..., h_l) \subseteq W\)&nbsp;e&nbsp;\(\alpha_i \in T^{-1}(h_i)\), allora&nbsp;\(T^{-1} (H) = \mathcal{L} (\alpha_1, ..., \alpha_l) \oplus ker(T)\)<br><b>Dimostrazione punto 2</b>:<br>(\(\Rightarrow\)) Se \(ker(T)=\{0\}\) e \(T(v_1)=T(v_2)\), allora \(T(v_1-v_2)=0\), quindi \(v_1-v_2 \in ker(T) \Rightarrow v_1-v_2=0 \Rightarrow v_1=v_2\).<br>(\(\Leftarrow\)) Se \(T\) è iniettiva e \(v \in ker(T)\), poiché \(T(0)=0\), allora deve essere \(v=0\).<br><b>Dimostrazione punto 3</b>:<br>Siano&nbsp;\(\alpha, \beta \in T^{-1} (w)\). Allora&nbsp;\(T(\beta - \alpha) = T(\beta) - T(\alpha) = w - w = \mathcal{O}_W\), quindi&nbsp;\(\beta - \alpha \in ker(T)\). Dunque&nbsp;\(\beta = \alpha + (\beta - \alpha)\), ossia&nbsp;\(\beta \in \alpha + ker(T)\).	ApplicazioniLineari
Jm>]4I]lzh	Geometria ed Algebra Lineare	Descrivi le proprietà di un'applicazione lineare associata ad una matrice.	Data la funzione \(L_A: \mathbb{K}^n \rightarrow \mathbb{K}^m\) definita da \(L_A(x) = A \cdot x\):<br>1. La composizione di funzioni corrisponde al prodotto matriciale: \(L_B \circ L_A = L_{B \cdot A}\).<br>2. Il nucleo della funzione coincide con il nucleo della matrice: \(ker(L_A) = ker(A)\).<br>3. La controimmagine di un vettore \(b\) è l'insieme delle soluzioni del sistema lineare: \(L_A^{-1}(b) = Sol(A,b)\).<br>4. L'immagine della funzione è lo span delle colonne della matrice: \(L_A(\mathbb{K}^n) = \mathcal{L}(Col(A))\), quindi \(dim(Im(L_A)) = rk(A)\).	ApplicazioniLineari
vxWFZcvRAR	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema di nullità più rango. Che conseguenze comporta?	<b>Teorema</b>: Sia \(T: V \rightarrow W\) lineare. Allora \(n = dim(V) = dim(T(V)) + dim(ker(T))\).<br><b>Dimostrazione</b>: Sia \(ker(T) = \cal L (u_1, \dots, u_l)\). Poiché&nbsp;\(T\)&nbsp;è lineare,&nbsp;\(T(V) = \cal L (T(u_1), \dots, T(u_l), T(v_{l + 1}), \dots, T(v_n)) = \cal L (0, \dots, 0, T(v_{l + 1}), \dots, T(v_n))\). Completiamo la base del kernel a una base di \(V\) aggiungendo \(\{v_{l+1}, ..., v_n\}\). Si dimostra che le immagini \(\{T(v_{l+1}), ..., T(v_n)\}\) sono linearmente indipendenti e generano l'immagine \(T(V)\), costituendone una base. Quindi \(dim(T(V)) = n - l = dim(V) - dim(ker(T))\).<br><b>Conseguenze</b>:<br><ul><li>Per una matrice \(A\), \(dim(ker(A)) = n - rk(A)\).</li><li>Se&nbsp;\(T\)&nbsp;è iniettiva, allora&nbsp;\(dim(V) = dim(T(V))\), il che è&nbsp;\(\leq dim(W)\), poiché&nbsp;\(ker(T) = {\mathcal{O}}\), quindi&nbsp;\(dim(ker(T)) = 0\)</li><li>Se&nbsp;\(T\)&nbsp;è suriettiva, allora&nbsp;\(dim(V) \ge dim(W)\), poiché&nbsp;\(T(V) = W\), quindi&nbsp;\(dim(V) = dim(W) + \underbrace{dim(ker(T))}_{\ge 0}\)</li></ul>	ApplicazioniLineari::Teoremi
I32{yC/hsy	Geometria ed Algebra Lineare	Quando una funzione lineare è biunivoca?	"Se \(T: V \rightarrow W\) e \(dim(V)=dim(W)\), allora le seguenti affermazioni sono equivalenti:<br>1. \(T\) è biunivoca (isomorfismo).<br>2. \(T\) è iniettiva (\(ker(T)=\{0\}\)).<br>3. \(T\) è suriettiva (\(dim(Im)=dim(W)\)).<br>4.&nbsp;\(V\)&nbsp;e&nbsp;\(W\)&nbsp;si dicono&nbsp;<i>spazi isomorfi</i><br>Basta verificarne una sola grazie al teorema di nullità più rango.
Se \(T: V \rightarrow W\) è una funzione lineare e \(dim(V)=dim(W)\), allora \(T\) è biunivoca se è iniettiva oppure se è suriettiva (è sufficiente verificarne una sola).<br><b>Dimostrazione</b>:<br>1. Se \(T\) è <b>iniettiva</b>, allora \(ker(T)=\{\mathcal{O}\}\) e \(dim(ker(T))=0\). Per il teorema di nullità più rango: \(dim(V) = dim(T(V)) + 0\). Dato che \(dim(V)=dim(W)\), si ha \(dim(T(V))=dim(W)\), il che implica che \(T\) è anche suriettiva.<br>2. Se \(T\) è <b>suriettiva</b>, allora \(T(V)=W\) e \(dim(T(V))=dim(W)\). Poiché \(dim(V)=dim(W)\), sostituendo nel teorema di nullità più rango si ottiene \(dim(V) = dim(V) + dim(ker(T))\), da cui \(dim(ker(T))=0\). Quindi \(T\) è anche iniettiva."	ApplicazioniLineari::Teoremi
Ip33]CvCid	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema di interpolazione di funzioni lineari.	<b>Enunciato</b>: Siano \(B=\{v_1, ..., v_n\}\) una base di \(V\) e \(w_1, ..., w_n\) vettori arbitrari di \(W\). Allora esiste ed è <b>unica</b> la funzione lineare \(F: V \rightarrow W\) tale che \(F(v_i) = w_i\) per ogni \(i=1,...,n\).<br><b>Dimostrazione</b>:<br>1. <b>Esistenza</b>: Ogni vettore \(v \in V\) si scrive in modo unico come \(v = \sum_{i=1}^n c_i v_i\). Definiamo la funzione \(F(v) = \sum_{i=1}^n c_i w_i\).<br>2. <b>Verifica Linearità</b>: Siano \(u, v \in V\) con coordinate \(a_i\) e \(b_i\) rispettivamente. Allora \(h u + k v\) ha coordinate \(h a_i + k b_i\).<br>Calcoliamo \(F(h u + k v) = \sum_{i=1}^n (h a_i + k b_i) w_i = h \sum a_i w_i + k \sum b_i w_i = h F(u) + k F(v)\). Quindi \(F\) è lineare.<br>3. <b>Condizione sulla base</b>: Calcoliamo \(F(v_j)\). Le coordinate di \(v_j\) rispetto alla base \(B\) sono \(c_i = 1\) se \(i=j\) e \(0\) altrimenti (vettore \(e_j\)).<br>Quindi \(F(v_j) = 0 \cdot w_1 + ... + 1 \cdot w_j + ... + 0 \cdot w_n = w_j\).<br>4. <b>Unicità</b>: Se esistesse un'altra funzione lineare \(G\) tale che \(G(v_i)=w_i\), per ogni \(v = \sum c_i v_i\) avremmo \(G(v) = \sum c_i G(v_i) = \sum c_i w_i = F(v)\).	ApplicazioniLineari::Teoremi
u]:T=iP]?W	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema di rappresentazione di funzioni lineari.	<b>Enunciato</b>: Sia \(T: V \rightarrow W\) un'applicazione lineare rispetto alle basi \(B=\{v_1, ..., v_n\}\) di \(V\) e \(B'\) di \(W\). Sia \(A = M_B^{B'}(T)\) la matrice rappresentativa. Valgono le seguenti proprietà:<br>1. <b>Relazione fondamentale</b>: \([T(v)]_{B'} = A \cdot [v]_B\) per ogni \(v \in V\).<br>2. <b>Nucleo</b>: \([ker(T)]_B = ker(A)\). Di conseguenza, \(dim(ker(T)) = n - rk(A)\).<br>3. <b>Immagine di sottospazi</b>: Se \(U = \mathcal{L}(u_1, ..., u_l) \subseteq V\), allora \([T(U)]_{B'} = \mathcal{L}(A \cdot [u_1]_B, ..., A \cdot [u_l]_B)\).<br>4. <b>Immagine totale</b>: \([T(V)]_{B'} = \mathcal{L}(Col(A))\). Di conseguenza, \(dim(T(V)) = rk(A)\).<br><br><b>Dimostrazioni</b>:<br><b>1.</b> Sia \(v = \sum c_i v_i\), quindi \([v]_B = (c_1, ..., c_n)^t\). Per linearità, \(T(v) = \sum c_i T(v_i)\). Passando alle coordinate rispetto a \(B'\) (isomorfismo): \([T(v)]_{B'} = \sum c_i [T(v_i)]_{B'}\). Questa somma corrisponde esattamente al prodotto della matrice \(A\) (le cui colonne sono \([T(v_i)]_{B'}\)) per il vettore colonna \([v]_B\).<br><b>2.</b> \(v \in ker(T) \iff T(v) = \mathcal{O}_W \iff [T(v)]_{B'} = [\mathcal{O}_W]_{B'} = \underline{0}\). Usando la proprietà 1, ciò equivale a \(A \cdot [v]_B = \underline{0}\), che è la definizione di appartenenza di \([v]_B\) al nucleo della matrice \(A\) (\(ker(A)\)).<br><b>3.</b> Sappiamo che \(T(U) = \mathcal{L}(T(u_1), ..., T(u_l))\). Passando alle coordinate in \(B'\), lo spazio è generato dalle coordinate dei generatori: \([T(U)]_{B'} = \mathcal{L}([T(u_1)]_{B'}, ..., [T(u_l)]_{B'})\). Sostituendo con la relazione 1, otteniamo \(\mathcal{L}(A \cdot [u_1]_B, ..., A \cdot [u_l]_B)\).<br><b>4.</b> Applichiamo la proprietà 3 al caso \(U=V\). Una base di \(V\) è \(B=\{v_1, ..., v_n\}\). Le coordinate dei vettori di base sono i vettori canonici: \([v_i]_B = e_i\). Dunque \([T(V)]_{B'} = \mathcal{L}(A \cdot e_1, ..., A \cdot e_n)\). Poiché \(A \cdot e_i\) restituisce la \(i\)-esima colonna di \(A\), lo spazio è generato dallo span delle colonne di \(A\) (\(Col(A)\)).	ApplicazioniLineari::Teoremi
H[Q.0r{N!=	Geometria ed Algebra Lineare	Enuncia e dimostra il criterio che determina se una funzione lineare è un isomorfismo.	<b>Criterio</b>: Una funzione lineare \(T\) è un isomorfismo se e solo se la sua matrice rappresentativa \(M_B^{B'}(T)\) è invertibile (quindi quadrata e con rango massimo).<br><b>Dimostrazione</b>: Poiché \(T^{-1} \circ T = I_V\), passando alle matrici si ha \(M_{B'}^B(T^{-1}) \cdot M_B^{B'}(T) = M_B^B(Id)\). Questo implica che \(M(T)\) deve essere invertibile e la sua inversa è la matrice rappresentativa della funzione inversa.<br>Consideriamo ora&nbsp;\(M(Id) = ([v_1]_B | \dots | [v_n]_B) = (e_1 | \dots | e_n) = I\). Quindi&nbsp;\(I = M_B^B (Id) = M_{B'}^B (T^{-1}) \cdot M_B^{B'}(T) \Leftrightarrow M_{B'}^B (T^{-1}) = (M_B^{B'}(T))^{-1}\)&nbsp;	ApplicazioniLineari
pn6S<|$w}&	Geometria ed Algebra Lineare	Cos'è una matrice di cambio base? Come posso calcolarla?	"La <b>matrice di cambio base</b> \(M_B^{B'}(I_V)\) è la matrice rappresentativa dell'identità. Le sue colonne sono le coordinate dei vettori della ""vecchia"" base \(B\) espressi rispetto alla ""nuova"" base \(B'\).<br><b>Calcolo</b>: Si costruisce la matrice affiancata \((w_1 ... w_n | v_1 ... v_n)\) (dove \(w_i\) sono i vettori di \(B'\) e \(v_i\) quelli di \(B\)) e si applica Gauss-Jordan per ridurla a \((I | M)\). La matrice \(M\) ottenuta è la matrice di cambio base."	ApplicazioniLineari
l2C2b8lR)+	Geometria ed Algebra Lineare	Enuncia il vero teorema di rappresentazione.	Il teorema afferma che l'applicazione che associa ad ogni funzione lineare la sua matrice rappresentativa (fissate le basi) è un <b>isomorfismo</b> di spazi vettoriali: \(M_B^{B'}: L(V,W) \rightarrow \mathbb{M}_{m,n}(\mathbb{K})\).<br>In particolare, preserva le operazioni: la matrice della somma di funzioni è la somma delle matrici e la matrice del prodotto per scalare è il prodotto dello scalare per la matrice: \(M(aT+bF) = aM(T)+bM(F)\).	ApplicazioniLineari::Teoremi
zG>~woWddW	Geometria ed Algebra Lineare	Cos'è il determinante di una matrice? Enuncia e dimostrane le proprietà.	Il <b>determinante</b> è una funzione \(det: \mathbb{M}_{n,n} \rightarrow \mathbb{K}\) che associa a una matrice quadrata uno scalare.<br><b>Proprietà e Dimostrazioni</b>:<br>1. <b>Multilineare</b>: è lineare su ciascuna riga fissate le altre.<br>2. <b>Alternante</b>: lo scambio di righe cambia segno. <i>Dimostrazione</i>: Sapendo che una matrice con due righe uguali ha determinante 0, calcoliamo \(det(..., R_i+R_j, ..., R_i+R_j, ...) = 0\). Espandendo per multilinearietà si ha \(det(R_i, R_i) + det(R_i, R_j) + det(R_j, R_i) + det(R_j, R_j) = 0\). I termini con righe uguali sono nulli, quindi resta \(det(R_i, R_j) + det(R_j, R_i) = 0 \Rightarrow det(R_i, R_j) = -det(R_j, R_i)\).<br>3. <b>Righe dipendenti</b>: se una riga è combinazione lineare delle altre, il determinante è 0. <i>Dimostrazione</i>: Espandendo per multilinearietà, si ottiene una somma di determinanti che hanno almeno due righe uguali, quindi tutti nulli.<br>4. <b>Invarianza mossa di Gauss</b> (\(R_i \rightarrow R_i + bR_j\)): non cambia il valore. <i>Dimostrazione</i>: \(det(..., R_i+bR_j, ..., R_j) = det(..., R_i, ..., R_j) + b \cdot det(..., R_j, ..., R_j)\). Il secondo termine ha due righe uguali, quindi è 0.<br>5. <b>Matrice triangolare</b>: prodotto degli elementi diagonali.	Determinante
i!Mz</^k=l	Geometria ed Algebra Lineare	La funzione determinante esiste sempre? Come posso calcolarlo?	La funzione determinante esiste ed è <b>unica</b>.<br><b>Metodi di calcolo</b>:<br>1. <b>Algoritmo di Gauss</b>: Si riduce la matrice a scala \(S\). Il determinante è il prodotto degli elementi diagonali di \(S\) moltiplicato per \((-1)^l\), dove \(l\) è il numero di scambi di riga effettuati.<br>2. <b>Sviluppo di Laplace</b>: \(det(A) = \sum_{j=1}^n (-1)^{i+j} a_{ij} \cdot det(\hat{A}_{ij})\), dove \(\hat{A}_{ij}\) è la sottomatrice ottenuta rimuovendo riga \(i\) e colonna \(j\).	Determinante
IX8xn:p/F/	Geometria ed Algebra Lineare	Enuncia il teorema di Binet.	Date due matrici quadrate \(A, B \in \mathbb{M}_{n,n}(\mathbb{K})\), il determinante del loro prodotto è uguale al prodotto dei loro determinanti:<br>\(det(A \cdot B) = det(A) \cdot det(B)\).	Determinante
BC*zPy_5K0	Geometria ed Algebra Lineare	Che conseguenze comporta il determinante?	1. Una matrice \(A\) è <b>invertibile</b> se e solo se \(det(A) \neq 0\). In tal caso, \(det(A^{-1}) = \frac{1}{det(A)}\).<br>2. Matrici simili hanno lo stesso determinante.<br>3. È utile per calcolare il rango (il rango è l'ordine massimo di un minore con determinante non nullo).	Determinante
p2$u@,,h0n	Geometria ed Algebra Lineare	Enuncia il teorema degli orlati.	"Il rango di una matrice \(A\) è \(p\) se e solo se:<br>1. Esiste una sottomatrice quadrata di ordine \(p\) (minore) con determinante diverso da 0.<br>2. Tutte le sottomatrici di ordine \(p+1\) ottenute ""orlando"" la precedente (aggiungendo una riga e una colonna della matrice originale) hanno determinante uguale a 0."	Determinante
"CVSNZ$G,#@"	Geometria ed Algebra Lineare	Come posso calcolare la matrice inversa utilizzando i complementi algebrici?	Se \(A\) è invertibile (\(det(A) \neq 0\)), la sua inversa è data da:<br>\(A^{-1} = \frac{1}{det(A)} (C)^t\),<br>dove \(C\) è la matrice dei <b>cofattori</b> (o complementi algebrici), con \(c_{ij} = (-1)^{i+j} det(\hat{A}_{ij})\). Bisogna quindi calcolare i cofattori, farne la trasposta e dividere per il determinante.	Determinante
bb8;U*s9~I	Geometria ed Algebra Lineare	Cos'è un endomorfismo diagonalizzabile? Cosa sono autovalori e autovettori?	1. Un endomorfismo \(T\) è <b>diagonalizzabile</b> se la sua matrice rappresentativa è simile a una matrice diagonale.<br>2. Un <b>autovettore</b> è un vettore \(u \neq \mathcal{O}\) tale che \(T(u) = \lambda u\) per uno scalare \(\lambda\).<br>3. Lo scalare \(\lambda\) è detto <b>autovalore</b>.	Diagonalizzazione
r.npE44P.@	Geometria ed Algebra Lineare	Enuncia il primo criterio di diagonalizzazione. Un endomorfismo è sempre diagonalizzabile?	<b>Primo criterio</b>: Un endomorfismo \(T: V \rightarrow V\) è diagonalizzabile se e solo se esiste una base di \(V\) composta interamente da autovettori di \(T\).<br>Non tutti gli endomorfismi sono diagonalizzabili (dipende dall'esistenza di sufficienti autovettori).	Diagonalizzazione
KIcrq1F)}$	Geometria ed Algebra Lineare	Cos'è lo spettro di una funzione lineare? Cos'è l'autospazio relativo ad un autovalore?	1. Lo <b>spettro</b> \(\sigma(T)\) è l'insieme degli autovalori distinti di \(T\).<br>2. L'<b>autospazio</b> \(V_{\lambda}\) relativo a un autovalore \(\lambda\) è l'insieme di tutti gli autovettori associati a \(\lambda\), più il vettore nullo: \(V_{\lambda} = \{v \in V : T(v) = \lambda v\}\).	Diagonalizzazione
t8`KrduFoM	Geometria ed Algebra Lineare	Enuncia e dimostra le proprietà di un autospazio.	Sia \(T: V \rightarrow V\) un endomorfismo e \(A = M_B^B(T)\) la sua matrice rappresentativa. Sia \(V_{\lambda}\) l'autospazio relativo a \(\lambda\).<br><b>1. Rappresentazione matriciale</b>: \([V_{\lambda}]_B = ker(A - \lambda I)\).<br><i>Dimostrazione</i>: \(u \in V_{\lambda} \iff T(u) = \lambda u\). Passando alle coordinate rispetto alla base \(B\): \([T(u)]_B = [\lambda u]_B\). Sfruttando la matrice rappresentativa: \(A \cdot [u]_B = \lambda \cdot [u]_B\). Portando tutto a sinistra: \(A \cdot [u]_B - (\lambda I) \cdot [u]_B = \underline{0} \iff (A - \lambda I) \cdot [u]_B = \underline{0}\). Questo equivale a dire che le coordinate di \(u\) appartengono al nucleo di \(A - \lambda I\).<br><br><b>2. Dimensione (Molteplicità geometrica)</b>: \(dim(V_{\lambda}) = n - rk(A - \lambda I)\).<br><i>Dimostrazione</i>: Poiché \(V_{\lambda}\) è isomorfo a \(ker(A - \lambda I)\), hanno la stessa dimensione. Per il Teorema di nullità più rango applicato alla matrice \(A - \lambda I\), si ha \(dim(ker) = n - rk(A - \lambda I)\).<br><br><b>3. Caratterizzazione dello spettro</b>: \(\lambda \in \sigma(T) \iff det(A - \lambda I) = 0\).<br><i>Dimostrazione</i>: \(\lambda\) è un autovalore \(\iff\) esiste un autovettore \(u \neq \mathcal{O}\) (quindi \(V_{\lambda} \neq \{\mathcal{O}\}\)) \(\iff\) il nucleo \(ker(A - \lambda I)\) contiene vettori non nulli (non è banale) \(\iff\) la matrice \(A - \lambda I\) non è invertibile (ha rango \(&lt; n\)) \(\iff det(A - \lambda I) = 0\).	Diagonalizzazione
"E1$s~c/w#R"	Geometria ed Algebra Lineare	Dimostra che esiste la somma diretta degli autospazi e che si può determinare una sua base.	Se \(\lambda_1, ..., \lambda_k\) sono autovalori distinti, la somma degli autospazi \(W = V_{\lambda_1} + ... + V_{\lambda_k}\) è diretta (\(\oplus\)) e una base è data dall'unione delle basi dei singoli autospazi.<br><b>Dimostrazione</b>:<br>1. <b>Indipendenza</b>: Sappiamo che autovettori relativi ad autovalori distinti sono linearmente indipendenti.<br>2. <b>Intersezione nulla</b>: Consideriamo due autospazi \(V_{\lambda_i}\) e \(V_{\lambda_j}\) con \(i \neq j\). Se esistesse un vettore \(u \neq \mathcal{O}\) nella loro intersezione, \(u\) sarebbe autovettore sia per \(\lambda_i\) che per \(\lambda_j\). Ma l'insieme \(\{u, u\}\) non può essere linearmente indipendente, contraddicendo il punto 1. Quindi \(V_{\lambda_i} \cap V_{\lambda_j} = \{\mathcal{O}\}\).<br>3. <b>Unione di basi</b>: Per il Corollario 4.5.14.1, se l'intersezione tra sottospazi è nulla, l'unione delle loro basi è un insieme linearmente indipendente.<br>4. <b>Iterazione</b>: Questo vale a due a due. Iterando il ragionamento, si dimostra che \((V_{\lambda_1} \oplus ... \oplus V_{\lambda_{k-1}}) \cap V_{\lambda_k} = \{\mathcal{O}\}\) (altrimenti avremmo una combinazione lineare di autovettori dipendente).<br>5. <b>Conclusione</b>: L'insieme \(B = \bigcup_{i=1}^k B_{V_{\lambda_i}}\) è formato da vettori linearmente indipendenti che generano la somma, quindi è una base.	Diagonalizzazione
"GKli}#opO5"	Geometria ed Algebra Lineare	Enuncia e dimostra il criterio geometrico di diagonalizzazione.	<b>Criterio</b>: \(T\) è diagonalizzabile se e solo se la somma delle molteplicità geometriche degli autovalori eguaglia la dimensione dello spazio: \(\sum m_g(\lambda_i) = dim(V)\).<br><b>Dimostrazione</b>: \(T\) è diagonalizzabile se esiste una base di autovettori (1° criterio). Poiché la somma diretta degli autospazi ha dimensione pari alla somma delle dimensioni (\(m_g\)), se questa somma copre l'intera dimensione \(n\) di \(V\), allora l'unione delle basi degli autospazi forma una base di \(V\) fatta di autovettori.	Diagonalizzazione
IopY1!wI(&	Geometria ed Algebra Lineare	Cos'è il polinomio caratteristico? Enuncia e dimostra il suo teorema.	Il <b>polinomio caratteristico</b> è \(p_A(x) = det(A-xI)\). È un polinomio di grado \(n\) le cui radici sono gli autovalori.<br><b>Teorema</b>: Il polinomio caratteristico non dipende dalla base scelta per rappresentare \(T\).<br><b>Dimostrazione</b>: Se \(B\) è simile ad \(A\) (\(B=P^{-1}AP\)), allora \(det(B-xI) = det(P^{-1}AP - xP^{-1}IP) = det(P^{-1}(A-xI)P) = det(P^{-1})det(A-xI)det(P) = det(A-xI)\).	Diagonalizzazione
oBeVN{%^xR	Geometria ed Algebra Lineare	Enuncia e dimostra le proprietà di un endomorfismo diagonalizzabile.	Se \(T\) è diagonalizzabile, la sua matrice è simile a una diagonale \(D = diag(\lambda_1, ..., \lambda_n)\).<br>1. Il polinomio caratteristico ha \(n\) radici nel campo (contate con molteplicità).<br>2. \(det(A) = det(D) = \prod \lambda_i\).<br>3. \(tr(A) = tr(D) = \sum \lambda_i\).<br><b>Dimostrazione</b>: Determinante e traccia sono invarianti per similitudine. Per una matrice diagonale, il determinante è il prodotto degli elementi diagonali e la traccia è la loro somma.	Diagonalizzazione
v&iogJ6eg6	Geometria ed Algebra Lineare	Se due matrici sono diagonalizzabili, cosa si ha di conseguenza?	Se due matrici \(A\) e \(B\) sono entrambe diagonalizzabili, esse sono simili (\(A \sim B\)) se e solo se hanno lo stesso polinomio caratteristico (ovvero gli stessi autovalori con le stesse molteplicità).	Diagonalizzazione
gLNrtawn{-	Geometria ed Algebra Lineare	Quando un autovalore si dice regolare?	Un autovalore \(\lambda\) è <b>regolare</b> se la sua molteplicità geometrica (\(m_g(\lambda) = dim(V_{\lambda})\)) coincide con la sua molteplicità algebrica (\(m_a(\lambda)\), molteplicità come radice del polinomio).<br>Vale sempre la disuguaglianza \(1 \le m_g(\lambda) \le m_a(\lambda)\).	Diagonalizzazione
zys?NZ[E8?	Geometria ed Algebra Lineare	Enuncia e dimostra il secondo criterio di diagonalizzabilità.	<b>Enunciato</b>: Sia \(T: V \rightarrow V\) con \(dim(V)=n\). \(T\) è diagonalizzabile se e solo se:<br>1. La somma delle molteplicità algebriche è \(n\) (\(\sum_{i=1}^l m_a(\lambda_i) = n\)).<br>2. Per ogni autovalore, la molteplicità geometrica eguaglia quella algebrica (\(m_g(\lambda_i) = m_a(\lambda_i)\)).<br><b>Dimostrazione</b>:<br>(\(\Leftarrow\)) Supponiamo che valgano le due condizioni. Allora \(\sum_{i=1}^l m_g(\lambda_i) = \sum_{i=1}^l m_a(\lambda_i) = n\). Per il <b>Criterio Geometrico</b>, se la somma delle dimensioni degli autospazi (\(m_g\)) è \(n\), allora \(T\) è diagonalizzabile.<br>(\(\Rightarrow\)) Supponiamo \(T\) diagonalizzabile. Per le proprietà degli endomorfismi diagonalizzabili, il polinomio caratteristico si decompone totalmente nel campo, quindi \(\sum m_a(\lambda_i) = n\).<br>Inoltre, per il Criterio Geometrico, sappiamo che \(\sum m_g(\lambda_i) = n\).<br>Sappiamo che per ogni autovalore vale \(m_g(\lambda_i) \le m_a(\lambda_i)\).<br>Procediamo per assurdo: se esistesse un \(\lambda_k\) tale che \(m_g(\lambda_k) &lt; m_a(\lambda_k)\), sommando su tutti gli indici avremmo:<br>\(\sum_{i=1}^l m_g(\lambda_i) &lt; \sum_{i=1}^l m_a(\lambda_i)\).<br>Sostituendo le somme note: \(n &lt; n\), che è un assurdo. Dunque deve essere necessariamente \(m_g(\lambda_i) = m_a(\lambda_i)\) per ogni \(i\).	Diagonalizzazione
f|]<=@0OV$	Geometria ed Algebra Lineare	Come posso verificare la diagonalizzabilità di un endomorfismo?	<b>Algoritmo</b>:<br>1. Calcolare il polinomio caratteristico \(p_T(x) = det(A-xI)\) e trovare le radici (spettro \(\sigma(T)\)).<br>2. Verificare se la somma delle molteplicità algebriche \(m_a\) è pari a \(n\) (se no, non è diagonalizzabile).<br>3. Per ogni autovalore \(\lambda\), calcolare \(m_g(\lambda) = n - rk(A-\lambda I)\).<br>4. Verificare se \(m_a(\lambda) = m_g(\lambda)\) per tutti gli autovalori. Se sì, è diagonalizzabile.<br>5. Per trovare la base diagonalizzante, unire le basi di tutti gli autospazi \(V_{\lambda}\).	Diagonalizzazione
cXH2/ea^}t	Geometria ed Algebra Lineare	Definisci il prodotto scalare.	Sia \( (V, +, \cdot, \mathbb{R}) \) uno spazio vettoriale. Si definisce <b>prodotto scalare</b> la funzione \( \langle \cdot, \cdot \rangle: V \times V \to \mathbb{R} \) che associa due vettori \( u, v \in V \) al numero \( \langle u, v \rangle \in \mathbb{R} \) e possiede le seguenti proprietà:<br>1. <b>Bilinearità</b>: \( \langle v + w, u \rangle = \langle u, w \rangle + \langle v, w \rangle \) e \( \langle c \cdot v, u \rangle = c \langle v, u \rangle = \langle v, c \cdot u \rangle, \forall v, u, w \in V, \forall c \in \mathbb{K} \).<br>2. <b>Simmetria</b>: \( \langle v, u \rangle = \langle u, v \rangle, \forall u, v \in V \).<br>3. <b>Definita positiva</b>: \( \langle v, v \rangle \ge 0, \forall v \in V \) e \( \langle v, v \rangle = 0 \iff v = \mathcal{O}_V \).<br>Lo spazio vettoriale arricchito dal prodotto scalare \( (V, +, \cdot, \mathbb{R}, \langle \cdot, \cdot \rangle) \) è detto <b>spazio euclideo</b>.	SpaziEuclidei
ncRY|^=.P^	Geometria ed Algebra Lineare	Enuncia le proprietà della norma di un vettore.	La norma di un vettore è definita come \( ||v|| = \sqrt{\langle v, v \rangle} \). Le sue proprietà sono:<br>1. <b>Omogeneità</b>: \( ||c \cdot v|| = |c| \cdot ||v|| \).<br><i>Dimostrazione</i>: \( ||c \cdot v||^2 = \langle c \cdot v, c \cdot v \rangle = c^2 \langle v, v \rangle = c^2 ||v||^2 \iff ||c \cdot v|| = |c| \cdot ||v|| \).<br>2. <b>Teorema di Carnot</b>: \( ||v \pm u||^2 = ||v||^2 \pm 2 \langle v, u \rangle + ||u||^2 \).<br><i>Dimostrazione</i>: Siano \( u, v \in V, t \in \mathbb{R} \). Allora \( ||u + tv||^2 = \langle u + tv, u + tv \rangle = \langle u, u \rangle + 2t \langle u, v \rangle + t^2 \langle v, v \rangle = ||u||^2 + 2t \langle u, v \rangle + t^2 ||v||^2 \).<br>3. <b>Disuguaglianza di Cauchy-Schwartz</b>: \( |\langle v, u \rangle| \le ||v|| \cdot ||u|| \).<br><i>Dimostrazione</i>: Considerando la relazione di Carnot come un polinomio in \( t \), abbiamo che \( t^2 ||v||^2 + 2t \langle v, u \rangle + ||u||^2 \ge 0, \forall t \in \mathbb{R} \) (poiché \( ||u+tv|| \ge 0 \)). Il polinomio non deve avere due radici distinte, quindi \( \Delta \le 0 \iff 4 \langle v, u \rangle^2 \le 4 ||v||^2 ||u||^2 \iff |\langle v, u \rangle| \le ||v|| \cdot ||u|| \).<br>4. <b>Disuguaglianza triangolare</b>: \( ||v + u|| \le ||v|| + ||u|| \).	SpaziEuclidei
yJvIz/j_.;	Geometria ed Algebra Lineare	Enuncia e dimostra le proprietà del prodotto scalare generalizzato.	Siano \( A \in \mathbb{M}_{n,n}(\mathbb{R}) \) e \( x, y \in \mathbb{R}^n \). Il prodotto scalare generalizzato \( \langle x, y \rangle = x^t A y \) ha le seguenti proprietà:<br>1. <b>È bilineare</b>.<br><i>Dimostrazione</i>: Sia \( z \in \mathbb{R}^n \). Allora \( \langle c_1 x + c_2 z, y \rangle = (c_1 x + c_2 z)^t A y = (c_1 x^t + c_2 z^t) A y = c_1 x^t A y + c_2 z^t A y = c_1 \langle x, y \rangle + c_2 \langle z, y \rangle \).<br>2. <b>Se A è simmetrica, allora lo è anche il prodotto scalare</b>.<br><i>Dimostrazione</i>: Poiché \( c = c^t, \forall c \in \mathbb{R} \), si ha \( \langle x, y \rangle = x^t A y = (x^t A y)^t = y^t (x^t A)^t = y^t A^t x \). Se \( A \) è simmetrica (\( A = A^t \)), allora \( = y^t A x = \langle y, x \rangle \).<br>3. In componenti: \( \langle x, y \rangle = \sum_{i,j=1}^n a_{ij} x_i y_j \).	SpaziEuclidei
Oe-OwG2(oi	Geometria ed Algebra Lineare	Cos'è la matrice di Gram? Come si può ridefinire il prodotto scalare grazie ad essa?	Sia \( (V, +, \cdot, \mathbb{R}, \langle \cdot, \cdot \rangle) \) uno spazio euclideo con \( B = \{v_1, \dots, v_n\} \) base di \( V \). Si dice <b>matrice di Gram</b> rispetto alla base \( B \) la matrice \( S_B \in \mathbb{M}_{n,n}(\mathbb{R}) \) tale che \( (S_B)_{ij} = \langle v_i, v_j \rangle \).<br>Grazie ad essa, dati \( u, w \in V \), il prodotto scalare si calcola come:<br>\( \langle u, w \rangle = [u]_B^t S_B [w]_B \).<br><i>Dimostrazione</i>: Siano \( u = \sum a_i v_i \) e \( w = \sum b_j v_j \). Allora \( \langle u, w \rangle = \langle \sum a_i v_i, \sum b_j v_j \rangle = \sum_{i,j} a_i b_j \langle v_i, v_j \rangle = \sum_{i,j} a_i b_j (S_B)_{ij} = [u]_B^t S_B [w]_B \).	SpaziEuclidei
6%<WQ5-c$	Geometria ed Algebra Lineare	"Quando due vettori si dicono ""ortogonali"" e quando ""ortonormali""?"	Siano \( v_i, v_j \) vettori di uno spazio euclideo. Essi si dicono:<br>- <b>Ortogonali</b> (\( \perp \)) se \( \langle v_i, v_j \rangle = 0 \).<br>- <b>Ortonormali</b> (o.n.) se sono ortogonali e hanno norma unitaria, ossia \( \langle v_i, v_j \rangle = \delta_{ij} \) (che vale 1 se \( i=j \), 0 se \( i \ne j \)).	SpaziEuclidei::Ortogonalità
M+2/},_Fut	Geometria ed Algebra Lineare	Cosa sono i coefficienti di Fourier? Quali sono le loro utilità?	I coefficienti di Fourier permettono di scrivere le coordinate di un vettore rispetto a una base ortogonale o ortonormale. Dato uno spazio euclideo con base ortogonale \( \{b_1, \dots, b_k\} \) e un vettore \( v \), l'i-esimo coefficiente è:<br>\( a_i = \frac{\langle v, b_i \rangle}{||b_i||^2} \).<br>Se la base è <b>ortonormale</b>, la formula si semplifica in \( a_i = \langle v, b_i \rangle \).<br>La loro utilità principale è trovare facilmente le coordinate di un vettore: \( [v]_B = \left( \frac{\langle v, b_1 \rangle}{||b_1||^2}, \dots, \frac{\langle v, b_n \rangle}{||b_n||^2} \right)^t \).	SpaziEuclidei::Ortogonalità
OK1cbHLjGG	Geometria ed Algebra Lineare	Cos'è il complemento ortogonale di un sottospazio? Enunciane e dimostra le proprietà.	Sia \( W \subseteq V \) un sottospazio vettoriale di uno spazio euclideo. Il <b>complemento ortogonale</b> di \( W \) è l'insieme \( W^\perp := \{ v \in V : \langle v, w \rangle = 0, \forall w \in W \} \).<br>Proprietà e dimostrazioni:<br>1. <b>\( W^\perp \) è un sottospazio vettoriale di V</b>.<br><i>Dimostrazione</i>: Siano \( u_1, u_2 \in W^\perp, c_1, c_2 \in \mathbb{R} \). \( \forall w \in W, \langle c_1 u_1 + c_2 u_2, w \rangle = c_1 \langle u_1, w \rangle + c_2 \langle u_2, w \rangle = 0 \implies c_1 u_1 + c_2 u_2 \in W^\perp \).<br>2. <b>Se \( \{w_1, \dots, w_m\} \) è base di W, allora \( v \in W^\perp \iff \langle v, w_i \rangle = 0, \forall i \)</b>.<br><i>Dimostrazione</i>: \( \langle v, w \rangle = \langle v, \sum a_i w_i \rangle = \sum a_i \langle v, w_i \rangle = 0 \implies v \in W^\perp \).<br>3. <b>\( (W^\perp)^\perp = W \)</b>.<br>4. <b>\( V = W \oplus W^\perp \)</b>.<br><i>Dimostrazione</i>: Sia \( v \in W \cap W^\perp \). Allora \( \langle v, v \rangle = 0 \implies v = \mathcal{O}_V \).	SpaziEuclidei::Ortogonalità
P*`[iMj{5p	Geometria ed Algebra Lineare	Cos'è la proiezione ortogonale su un sottospazio? Come posso riscrivere un vettore grazie ad essa? Enuncia e dimostrane le proprietà.	<b>Definizione</b>: Sia \( W \) un sottospazio di \( V \). Si dice proiezione ortogonale su \( W \) l'endomorfismo \( P_W: V \to V \) tale che:<br>1. \( P_W(V) = W \).<br>2. \( P_W(w) = w, \forall w \in W \) (o \( P_W \circ P_W = P_W \)).<br>3. \( v - P_W(v) \in W^\perp, \forall v \in V \).<br>Ogni vettore \( v \) può essere riscritto come somma di componenti: \( v = \underbrace{P_W(v)}_{\in W} + \underbrace{(v - P_W(v))}_{\in W^\perp} \).<br><br><b>Esistenza e formula</b>: Se \( \{b_1, \dots, b_m\} \) è base o.n. di \( W \), allora \( P_W(v) = \sum_{i=1}^m \langle v, b_i \rangle b_i \).<br><i>Dimostrazione</i>:<br>- (2) Se \( w \in W, w = \sum c_k b_k \), allora \( P_W(w) = \sum c_k P_W(b_k) \). Poiché \( b_k \in W \), \( P_W(b_k) = \sum_j \langle b_k, b_j \rangle b_j = 1 \cdot b_k = b_k \).<br>- (1) \( P_W(v) \) è combinazione lineare di vettori di \( W \), quindi appartiene a \( W \).<br>- (3) \( \langle v - P_W(v), b_i \rangle = \langle v, b_i \rangle - \sum_j \langle v, b_j \rangle \langle b_j, b_i \rangle = \langle v, b_i \rangle - \langle v, b_i \rangle = 0 \).<br><br><b>Proprietà</b>:<br>1. \( ker(P_W) = W^\perp \).<br><i>Dimostrazione</i>: Sia \( v \in ker(P_W) \). \( P_W(v) = \sum \langle v, b_i \rangle b_i = 0 \iff \langle v, b_i \rangle = 0 \forall i \iff v \in W^\perp \).<br>2. \( ||v - w|| &gt; ||v - P_W(v)|| \) per ogni \( w \in W, w \ne P_W(v) \) (è la distanza minima).<br><i>Dimostrazione</i>: \( v - w = (v - P_W(v)) + (P_W(v) - w) \). Poniamo \( u_1 = v - P_W(v) \in W^\perp \) e \( u_2 = P_W(v) - w \in W \). Per Pitagora: \( ||v-w||^2 = ||u_1||^2 + ||u_2||^2 &gt; ||u_1||^2 \).	SpaziEuclidei::Ortogonalità
f-2slE(c~q	Geometria ed Algebra Lineare	A cosa serve l'algoritmo di Gram-Schmidt e come procede?	L'algoritmo di Gram-Schmidt serve a generare una base ortonormale \( \{b_1, \dots, b_m\} \) a partire da un insieme di vettori linearmente indipendenti \( \{w_1, \dots, w_m\} \).<br>L'algoritmo procede così:<br>1. \( b_1 = \frac{w_1}{||w_1||} \).<br>2. Calcola \( u_2 = w_2 - P_{\mathcal{L}(b_1)}(w_2) = w_2 - \langle w_2, b_1 \rangle b_1 \). Poi \( b_2 = \frac{u_2}{||u_2||} \).<br>3. Calcola \( u_3 = w_3 - P_{\mathcal{L}(b_1, b_2)}(w_3) = w_3 - (\langle w_3, b_1 \rangle b_1 + \langle w_3, b_2 \rangle b_2) \). Poi \( b_3 = \frac{u_3}{||u_3||} \).<br>...<br>m. Calcola \( u_m = w_m - P_{U_{m-1}}(w_m) = w_m - \sum_{j=1}^{m-1} \langle w_m, b_j \rangle b_j \). Poi \( b_m = \frac{u_m}{||u_m||} \).	SpaziEuclidei::Ortogonalità
O{n(8N;7U]	Geometria ed Algebra Lineare	Cos'è la distanza tra due sottospazi affini? Come posso definirla in modo alternativo?	Dati due sottospazi affini \( A_1, A_2 \), la distanza è \( d(A_1, A_2) = \min \{ ||P_1 - P_2|| : P_1 \in A_1, P_2 \in A_2 \} \).<br>Alternativamente, se \( A_1 = P_1 + W_1 \) e \( A_2 = P_2 + W_2 \), la distanza è data da \( ||P_{U^\perp}(P_1 - P_2)|| \), dove \( U = W_1 + W_2 \).	SpaziEuclidei::TrasformazioniAffini
"ssrLJuMM#f"	Geometria ed Algebra Lineare	Cos'è una trasformazione affine?	Una <b>trasformazione affine</b> è una funzione \( f: V \to V \) definita come \( f(v) = v_0 + T(v) \), dove \( v_0 \in V \) è un vettore di traslazione e \( T: V \to V \) è un endomorfismo. È la composizione di un'applicazione lineare e una traslazione.	SpaziEuclidei::TrasformazioniAffini
Oj*D0EU=e;	Geometria ed Algebra Lineare	Cos'è un'isometria lineare? Enunciane e dimostrane le proprietà.	Un endomorfismo \( T: V \to V \) è un'<b>isometria lineare</b> se conserva il prodotto scalare: \( \langle T(v), T(u) \rangle = \langle v, u \rangle, \forall u, v \in V \).<br>Proprietà:<br>1. <b>Le norme si preservano</b>: \( ||T(v)|| = ||v|| \).<br><i>Dimostrazione</i>: \( ||T(v)||^2 = \langle T(v), T(v) \rangle = \langle v, v \rangle = ||v||^2 \).<br>2. <b>T è un isomorfismo (quindi iniettiva)</b>.<br><i>Dimostrazione</i>: Sia \( v \in ker(T) \). Allora \( 0 = ||\mathcal{O}_V|| = ||T(v)|| = ||v|| \implies v = \mathcal{O}_V \implies ker(T) = \{\mathcal{O}_V\} \).<br>3. <b>Gli angoli si preservano</b>: \( \cos \theta = \frac{\langle v, u \rangle}{||v|| ||u||} = \frac{\langle T(v), T(u) \rangle}{||T(v)|| ||T(u)||} \).	SpaziEuclidei::Isometrie
v$i6=ZLfPP	Geometria ed Algebra Lineare	Come posso determinare se un'endomorfismo è un'isometria lineare? Fornisci la dimostrazione.	Sia \( T \) un endomorfismo e \( B, B' \) basi ortonormali. \( T \) è un'isometria lineare se e solo se la sua matrice rappresentativa \( M_B^{B'}(T) = (C_1 | \dots | C_n) \) ha le colonne che formano una base ortonormale (ovvero \( C_i^t C_j = \delta_{ij} \)).<br><i>Dimostrazione</i>: Poiché \( B' \) è o.n., \( S_{B'} = I \), quindi \( \langle u, v \rangle = [u]_{B'}^t [v]_{B'} \).<br>Considerando le colonne della matrice, \( C_i^t \cdot C_j = [T(b_i)]_{B'}^t \cdot [T(b_j)]_{B'} = \langle T(b_i), T(b_j) \rangle \).<br>Se \( T \) è un'isometria, \( \langle T(b_i), T(b_j) \rangle = \langle b_i, b_j \rangle = \delta_{ij} \) (poiché \( B \) è o.n.).	SpaziEuclidei::Isometrie
MzxQ0KmQXd	Geometria ed Algebra Lineare	Cos'è una matrice ortogonale? Com'è strutturata? Fornisci la dimostrazione.	Una matrice \( A \in \mathbb{M}_{n,n}(\mathbb{R}) \) si dice <b>ortogonale</b> (\( A \in O(n) \)) se le sue colonne formano una base ortonormale di \( \mathbb{R}^n \).<br>Struttura: \( A \) è invertibile e \( A^{-1} = A^t \), ovvero \( A \cdot A^t = A^t \cdot A = I \).<br><i>Dimostrazione</i>: Sia \( A = (C_1 | \dots | C_n) \). Allora \( (A^t A)_{ij} = C_i^t C_j \). Poiché le colonne sono o.n., \( C_i^t C_j = \delta_{ij} \), quindi \( A^t A = I \).	SpaziEuclidei::Isometrie
dR[INXD-4m	Geometria ed Algebra Lineare	Cos'è un'isometria?	Un'applicazione \( f: V \to V \) è un'<b>isometria</b> se conserva le distanze: \( ||u - v|| = ||f(u) - f(v)||, \forall u, v \in V \).<br>Ogni isometria può essere scritta come \( f(v) = T(v) + v_0 \), dove \( T \) è un'isometria lineare.	SpaziEuclidei::Isometrie
dit}E]e0CO	Geometria ed Algebra Lineare	"Quando un endomorfismo si dice ""simmetrico""?"	Un endomorfismo \( T: V \to V \) su uno spazio euclideo si dice <b>simmetrico</b> se \( \langle T(u), v \rangle = \langle u, T(v) \rangle, \forall u, v \in V \).<br>Se la base \( B \) scelta è ortonormale, la matrice associata a un endomorfismo simmetrico è una matrice simmetrica (\( A = A^t \)).	SpaziEuclidei::EndomorfismiSimmetrici
m)oLuIfL0}	Geometria ed Algebra Lineare	"Cos'è un endomorfismo ""ortogonalmente diagonalizzabile""? Che ulteriore proprietà possiede?"	Un endomorfismo \( T \) è <b>ortogonalmente diagonalizzabile</b> se esiste una base ortonormale di \( V \) composta da autovettori di \( T \).<br>Se \( T \) è ortogonalmente diagonalizzabile, allora \( T \) è necessariamente <b>simmetrico</b>.	SpaziEuclidei::EndomorfismiSimmetrici
JYA9}ry&c|	Geometria ed Algebra Lineare	Come sono fatti gli autospazi distinti di un endomorfismo simmetrico?	Se \( T \) è un endomorfismo simmetrico e \( \lambda, \mu \in \sigma(T) \) con \( \lambda \ne \mu \), allora i relativi autospazi sono ortogonali: \( V_\lambda \perp V_\mu \).<br><i>Dimostrazione</i>: Siano \( u \in V_\lambda, v \in V_\mu \). \( \lambda \langle u, v \rangle = \langle \lambda u, v \rangle = \langle T(u), v \rangle = \langle u, T(v) \rangle \) (per simmetria) \( = \langle u, \mu v \rangle = \mu \langle u, v \rangle \).<br>Quindi \( (\lambda - \mu) \langle u, v \rangle = 0 \). Poiché \( \lambda \ne \mu \), deve essere \( \langle u, v \rangle = 0 \).	SpaziEuclidei::EndomorfismiSimmetrici
i})Zf@}jBW	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema spettrale.	<b>Teorema 8.6.2</b>: Sia \( T: V \to V \) un endomorfismo. \( T \) è simmetrico se e solo se è ortogonalmente diagonalizzabile.<br><i>Dimostrazione</i>:<br>(\( \Leftarrow \)) Se \( T \) è ortogonalmente diagonalizzabile, esiste una base o.n. \( B \) tale che \( M_B^B(T) \) è diagonale. Una matrice diagonale è simmetrica (\( D = D^t \)). Poiché \( B \) è o.n., \( M_B^B(T) \) simmetrica implica \( T \) simmetrico (Teorema 8.5.2).<br>(\( \Rightarrow \)) Supponiamo \( T \) simmetrico. Procediamo per induzione su \( n = dim(V) \).<br>1. Se \( n=1 \), \( V = \mathcal{L}(v_1) \). \( T(v_1) = k v_1 \). Ponendo \( b_1 = v_1 / ||v_1|| \), otteniamo una base o.n. di autovettori.<br>2. Supponiamo che ogni endomorfismo simmetrico su uno spazio di dimensione \( n-1 \) sia ortogonalmente diagonalizzabile.<br>3. Per il Lemma 8.5.4, esiste almeno un autovalore reale \( \lambda \). Sia \( b_1 \) un autovettore unitario relativo a \( \lambda \).<br>4. Sia \( U = \mathcal{L}(b_1) \) e \( W = U^\perp \). Sappiamo che \( T(W) \subseteq W \).<br><i>Motivazione</i>: \( \forall w \in W, \langle w, b_1 \rangle = 0 \). Allora \( \langle T(w), b_1 \rangle = \langle w, T(b_1) \rangle \) (per simmetria) \( = \langle w, \lambda b_1 \rangle = \lambda \langle w, b_1 \rangle = 0 \). Quindi \( T(w) \perp b_1 \implies T(w) \in W \).<br>5. \( V = W \oplus U \) e \( dim(W) = n-1 \). La restrizione \( T|_W: W \to W \) è ancora simmetrica.<br>6. Per ipotesi induttiva, esiste una base o.n. \( \{b_2, \dots, b_n\} \) di \( W \) fatta di autovettori.<br>7. Poiché \( b_1 \in W^\perp \), \( b_1 \) è ortogonale a \( b_2, \dots, b_n \). Dunque \( \{b_1, \dots, b_n\} \) è una base o.n. di \( V \) di autovettori.	SpaziEuclidei::TeoremaSpettrale
FZOCOSVeU-	Geometria ed Algebra Lineare	Come posso trovare una base ortonormale fatta di autovettori?	Dato un endomorfismo simmetrico \( T: V \to V \) e una base o.n. \( B \) di \( V \):<br>1. Costruisci la matrice \( A = M_B^B(T) \). Se \( A = A^t \), allora \( T \) è simmetrico e quindi ortogonalmente diagonalizzabile (Teorema spettrale).<br>2. Calcola lo spettro \( \sigma(T) = \{\lambda_1, \dots, \lambda_l\} \) trovando le radici del polinomio caratteristico.<br>3. Per ogni autovalore \( \lambda_i \), calcola una base \( B_i = \{u_1, \dots, u_k\} \) dell'autospazio \( V_{\lambda_i} \) risolvendo il sistema omogeneo \( (A - \lambda_i I)v = 0 \).<br>4. Usa l'algoritmo di <b>Gram-Schmidt</b> su ciascuna base \( B_i \) per ottenere una base ortonormale \( B'_i = \{v_1, \dots, v_k\} \) per quell'autospazio.<br>5. Considera l'unione di queste basi: \( B' = \bigcup_{i=1}^l B'_i \). Questa è la base ortonormale di \( V \) fatta di autovettori cercata.<br>6. La matrice di cambio base \( P \) formata dalle colonne di \( B' \) sarà ortogonale (\( P^{-1} = P^t \)) e tale che \( P^t A P \) è diagonale.	SpaziEuclidei::TeoremaSpettrale
ilL;)Z_*f}	Geometria ed Algebra Lineare	Enuncia il teorema di decomposizione spettrale.	Sia \( T: V \to V \) un endomorfismo simmetrico con autovalori \( \lambda_1, \dots, \lambda_l \) e siano \( P_{V_{\lambda_i}} \) le proiezioni ortogonali sui relativi autospazi. Allora:<br>\( T = \sum_{i=1}^l \lambda_i P_{V_{\lambda_i}} \) e \( M_B^B(T) = \sum_{i=1}^l \lambda_i M_B^B(P_{V_{\lambda_i}}) \).	SpaziEuclidei::TeoremaSpettrale
IfkX6TY&rJ	Geometria ed Algebra Lineare	Cos'è una forma quadratica?	Una funzione \(q(x): \mathbb{R}^n \rightarrow \mathbb{R}\) definita come la somma di monomi solamente di 2° grado: \(q(x) = \sum_{i,j=1}^n a_{ij}x_i x_j\).<br>Data una forma quadratica, esiste sempre una matrice simmetrica \(B\) tale che \(q(x) = x^t B x\), definita come \(b_{ij} = \frac{a_{ij}+a_{ji}}{2}\).<br>Deduciamo inoltre che \(q(\underline{0}) = 0\) e \(q(tx) = t^2 q(x)\).	FormeQuadratiche
tFj_[dB&5q	Geometria ed Algebra Lineare	Come si determina il segno di una forma quadratica?	Una forma quadratica \(q(x) = x^t B x\) con \(B=B^t\) si dice:<br><ul><li><b>Definita positiva:</b> se \(q(x) &gt; 0, \forall x \in \mathbb{R}^n \setminus \{\underline{0}\}\).</li><li><b>Definita negativa:</b> se \(q(x) &lt; 0, \forall x \in \mathbb{R}^n \setminus \{\underline{0}\}\).</li><li><b>Semidefinita positiva:</b> se \(q(x) \ge 0, \forall x \in \mathbb{R}^n\) e \(\exists y \in \mathbb{R}^n \setminus \{\underline{0}\} : q(y) = 0\).</li><li><b>Semidefinita negativa:</b> se \(q(x) \le 0, \forall x \in \mathbb{R}^n\) e \(\exists y \in \mathbb{R}^n \setminus \{\underline{0}\} : q(y) = 0\).</li><li><b>Indefinita:</b> se \(\exists x, y \in \mathbb{R}^n : q(x) &lt; 0, q(y) &gt; 0\).</li></ul>	FormeQuadratiche
"gfr@aHV9.#"	Geometria ed Algebra Lineare	Cos'è la segnatura di una forma quadratica? Cosa possiamo dedurre da essa?	Si dice segnatura di \(q(x)\) la terna \((n_+, n_-, n_0)\) dove:<br><ul><li>\(n_+\) è la dimensione massima del sottospazio \(V_+ \subseteq \mathbb{R}^n\) tale che \(q(x) &gt; 0, \forall x \in V_+ \setminus \{\underline{0}\}\).</li><li>\(n_-\) è la dimensione massima del sottospazio \(V_- \subseteq \mathbb{R}^n\) tale che \(q(x) &lt; 0, \forall x \in V_- \setminus \{\underline{0}\}\).</li><li>\(n_0 = n - (n_+ + n_-)\).</li></ul><br>Dalla segnatura deduciamo il segno della forma quadratica:<br><ul><li>\((n, 0, 0)\): definita positiva.</li><li>\((0, n, 0)\): definita negativa.</li><li>\((a, 0, b)\) con \(b \ne 0\): semidefinita positiva.</li><li>\((0, a, b)\) con \(b \ne 0\): semidefinita negativa.</li><li>\((a, b, c)\) con \(a, b \ne 0\): indefinita.</li></ul>	FormeQuadratiche
hTM2qz?9l2	Geometria ed Algebra Lineare	Cos'è la forma normale di Sylvester? Enuncia e dimostra il teorema di inerzia di Sylvester.	La forma normale di Sylvester è la matrice \(S = \text{diag}(I_p, -I_k, 0I_0)\).<br><b>Teorema di Inerzia:</b> Sia \(B \in \mathbb{M}_{n,n}(\mathbb{R})\) simmetrica. Allora:<br>1. \(B\) è congruente alla matrice \(S\), dove \(p\) e \(k\) sono il numero di autovalori positivi e negativi.<br>2. La segnatura di \(q(x)\) è \((p, k, m_a(0))\).<br>3. Un possibile sottospazio massimale \(V_+\) è dato dalla somma diretta degli autospazi degli autovalori positivi (\(V_+ = V_{\lambda_1} \oplus ... \oplus V_{\lambda_p}\)).<br><br><b>Dimostrazione:</b><br>1. Poiché \(B\) è simmetrica, \(\exists P \in O(n)\) tale che \(P^t B P = \text{diag}(\lambda_1, ..., \lambda_p, \lambda_{p+1}, ..., 0) = D\). Consideriamo la matrice \(F = \text{diag}(\frac{1}{\sqrt{|\lambda_1|}}, ..., \frac{1}{\sqrt{|\lambda_{p+k}|}}, ..., 1)\). Allora \(F^t D F = S\), poiché \(\frac{\lambda_i}{(\sqrt{|\lambda_i|})^2} = \pm 1\). Ponendo \(C = PF\), abbiamo che \(C^t B C = S\), quindi \(B\) è congruente a \(S\).<br>2. Segue dal teorema di equivalenza delle segnature per matrici congruenti.<br>3. Siano \(u_i \in V_{\lambda_i}\) e \(v = u_1 + ... + u_p \ne \mathcal{O}_V\). Per l'ortogonalità degli autospazi, \(u_i^t u_j = 0\) se \(i \ne j\). Quindi \(q(v) = \lambda_1 ||u_1||^2 + ... + \lambda_p ||u_p||^2\). Poiché \(\lambda_i &gt; 0\), allora \(q(v) &gt; 0\), quindi&nbsp;\(v \in V_+ \Leftrightarrow V_+ = V_{\lambda_1} \oplus ... \oplus V_{\lambda_p} \).	FormeQuadratiche
A1,zO1%hIp	Geometria ed Algebra Lineare	Cos'è un'ipersuperficie quadrica? Com'è composta e come si può rappresentare?	È il sottoinsieme \(Q = \{ x \in \mathbb{R}^n : q(x) = 0 \}\) dove \(q(x)\) è un polinomio di 2° grado.<br>L'espressione è composta da una <b>forma quadratica</b>, una <b>parte lineare</b> e una <b>costante</b>.<br>In forma matriciale:<br>1. \(q(x) = x^t B x + 2b^t x + c = 0\) dove \(b \in \mathbb{R}^n\).<br>2. \( (x_1 \dots x_n \ 1) \begin{pmatrix} B &amp; b \\ b^t &amp; c \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \\ 1 \end{pmatrix} = 0 \).	FormeQuadratiche::Quadriche
.VnrTsp^-	Geometria ed Algebra Lineare	Grazie a cosa è possibile classificare coniche e quadriche? Che tipi di classificazione esistono?	È possibile classificarle grazie a una trasformazione di coordinate (trasformazione affine) \(f: \mathbb{R}^n \rightarrow \mathbb{R}^n\) definita come \(f(x) = Px - v_0'\), che permette di passare a un sistema di riferimento che semplifica l'equazione.<br>Esistono due tipi di classificazione:<br>1. <b>Classificazione affine:</b> se \(P\) è solo invertibile.<br>2. <b>Classificazione metrica:</b> se \(P\) è anche ortogonale (isometria). Con questa classificazione non si hanno deformazioni.	FormeQuadratiche::Quadriche
wm+4~v1967	Geometria ed Algebra Lineare	Quando una quadrica presenta una simmetria centrale? Come posso trovare il potenziale centro di una quadrica?	Una quadrica \(Q\) ha simmetria centrale se \(q(x) = q(-x)\), il che avviene se la parte lineare è nulla o può essere eliminata (cioè se l'insieme delle soluzioni \(Sol(B, -b) \ne \emptyset\)).<br>Per trovare il centro \(v_0\) (o l'insieme dei centri \(\epsilon(Q)\)), bisogna risolvere il sistema lineare:<br>\[ B v_0 = -b \]	FormeQuadratiche::Quadriche
oJ}U/@wJE,	Geometria ed Algebra Lineare	Qual è la forma canonica metrica di una quadrica a centro? Come posso determinarla?	La forma canonica è: \[\begin{cases} \lambda_1 z_1^2 + \dots + \lambda_r z_r^2 = 0 &amp; \text{se } \tilde{c} = 0 \\ \frac{\lambda_1}{\tilde{c}} z_1^2 + \dots + \frac{\lambda_r}{\tilde{c}} z_r^2 + 1 = 0 &amp; \text{se } \tilde{c} \ne 0 \end{cases}\] con \(r=rk(B)\).<br><b>Procedimento di derivazione:</b><br>1. <b>Traslazione (eliminazione parte lineare):</b> Si trova un centro \(v_0\) risolvendo \(Bv_0 = -b\). Si applica la traslazione \(y = x - v_0\) che trasforma l'equazione in \(y^t B y + \tilde{c} = 0\), dove \(\tilde{c} = b^t v_0 + c\).<br>2. <b>Rotazione (diagonalizzazione):</b> Poiché \(B\) è simmetrica, esiste una matrice ortogonale \(P\) (composta da autovettori normalizzati) tale che \(P^t B P = D\) (diagonale). Si applica l'isometria \(y = Pz\).<br>3. L'equazione diventa \(z^t D z + \tilde{c} = 0\), ossia \(\lambda_1 z_1^2 + \dots + \lambda_n z_n^2 + \tilde{c} = 0\)	FormeQuadratiche::Quadriche
Dq}H7z~u}%	Geometria ed Algebra Lineare	Cos'è l'asse di una quadrica non a centro? Com'è possibile determinarlo se il sistema Bv = -b è impossibile?	L'asse di una quadrica \(Q\) non a centro è l'insieme \(Ax(Q) = v_0 + ker(B)\).<br>Poiché il sistema \(Bv = -b\) è impossibile (quindi \(Q\) non ha centro), si considera la proiezione \(P_W(b)\) su \(W = \mathcal{L}(Col(B))\).<br>L'asse è quindi:<br>\[ Ax(Q) = Sol(B, -P_W(b)) = Sol(B^2, -Bb) = v_0 + ker(B) \]	FormeQuadratiche::Quadriche
FX]I|gwnBU	Geometria ed Algebra Lineare	Qual è la forma canonica metrica di una quadrica non a centro? Come posso determinarla?	"La forma canonica è: \[\lambda_1 w_1^2 + ... + \lambda_r w_r^2 + 2 ||b_0|| w_{r+1} = 0\] dove \(b_0 = P_{ker(B)}(b)\).<br><b>Procedimento di derivazione:</b><br>1. <b>Approssimazione e Traslazione:</b> Poiché il centro non esiste, si cerca l'asse \(Ax(Q)\) risolvendo il ""sistema approssimato""&nbsp;\(Bv_0 = -P_{Im(B)}(b)\).<br>Si applica la traslazione \(y = x - v_0\) che riduce l'equazione a \(y^t B y + 2y^t b_0 + \tilde{c} = 0\).<br>2. <b>Rotazione mirata:</b> Si costruisce una base ortonormale \(B'\) che include gli autovettori di \(B\) e, per il nucleo, il vettore \(b_{r+1} = \frac{b_0}{||b_0||}\). <br>Applicando \(y = Pz\), l'equazione diventa \(\sum \lambda_i z_i^2 + 2||b_0||z_{r+1} + \tilde{c} = 0\).<br>3. <b>Traslazione finale (eliminazione costante):</b> Si trasla lungo l'asse \(z_{r+1}\) ponendo \(w = z + (0 \dots \frac{\tilde{c}}{2||b_0||} \dots 0)\) per eliminare \(\tilde{c}\)."	FormeQuadratiche::Quadriche
OCY$S[H8@(	Geometria ed Algebra Lineare	Dato uno spazio vettoriale&nbsp;\(V\)&nbsp;e fissata una sua <b>base</b>&nbsp;\(\mathcal{B} = \{v_1, \dots, v_n\}\), cosa vale per la scrittura di un qualsiasi vettore&nbsp;\(v \in V\)&nbsp;come combinazione lineare degli elementi della base?	Tale scrittura esiste ed è <b>unica</b>.<br><br>Cioè, esistono dei coefficienti scalari unici&nbsp;\(\alpha_1, \dots, \alpha_n\)&nbsp;tali che&nbsp;\(v = \alpha_1 v_1 + \dots + \alpha_n v_n\)<br>Questa unicità è garantita dal fatto che i vettori della base sono linearmente indipendenti.	Spazi::Basi
m0[q;32L{	Geometria ed Algebra Lineare	"Enuncia il Lemma di Steinitz (o ""Troppi vettori sono linearmente dipendenti"")"	Se \(\mathcal{L}(v_{1},...,v_{n})=V\) allora \(w_{1},...,w_{m}\in V\) con \(m&gt;n\) sono <b>linearmente dipendenti</b>.	Spazi::Basi
ORLKmSow~6	Geometria ed Algebra Lineare	Enuncia e dimostra il teorema della dimensione.	<b>Enunciato:</b><br>Siano \(B_{1}=\{v_{1},...,v_{m}\}\) e \(B_{2}=\{w_{1},...,w_{n}\}\) due basi di uno spazio vettoriale \(V\) finitamente generato.<br>Allora \(n=m\).<br><br><b>Dimostrazione:</b><br>Supponiamo, per assurdo, che \(n &gt; m\).<br>Poiché \(B_1\) genera \(V\) ed ha \(m\) elementi, se prendiamo gli \(n\) vettori di \(B_2\) (con \(n &gt; m\)), questi devono essere <b>linearmente dipendenti</b> secondo il <em>Lemma di Steinitz.</em><br>Questo è un <b>assurdo</b> perché, per ipotesi, \(B_2\) è una base, quindi i suoi vettori devono essere linearmente indipendenti.<br>Dunque, necessariamente, \(n=m\).	Spazi::Basi
